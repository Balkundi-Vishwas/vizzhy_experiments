{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf4e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ingredients_from_html(html_content, recipe_name=None):\n",
    "    \"\"\"Extract recipe ingredients from HTML using the LLM\"\"\"\n",
    "    # Preprocess HTML to focus on likely ingredient sections\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove scripts, styles, and comments\n",
    "    for element in soup(['script', 'style']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Try to find ingredient sections\n",
    "    ingredient_sections = []\n",
    "    \n",
    "    # Look for common ingredient section identifiers\n",
    "    ingredient_indicators = ['ingredient', 'ingredients']\n",
    "    for indicator in ingredient_indicators:\n",
    "        # Check for elements with ingredient-related classes or IDs\n",
    "        for element in soup.find_all(class_=lambda c: c and indicator in c.lower()):\n",
    "            ingredient_sections.append(element.get_text())\n",
    "        \n",
    "        for element in soup.find_all(id=lambda i: i and indicator in i.lower()):\n",
    "            ingredient_sections.append(element.get_text())\n",
    "        \n",
    "        # Look for lists near headers with ingredient text\n",
    "        for header in soup.find_all(['h1', 'h2', 'h3', 'h4']):\n",
    "            if indicator in header.get_text().lower():\n",
    "                # Get the next few elements\n",
    "                ingredients_list = []\n",
    "                next_element = header.find_next()\n",
    "                \n",
    "                # Collect up to 20 elements after the header\n",
    "                count = 0\n",
    "                while next_element and count < 20:\n",
    "                    if next_element.name == 'ul' or next_element.name == 'ol':\n",
    "                        ingredients_list.append(next_element.get_text())\n",
    "                        break\n",
    "                    elif next_element.name == 'li':\n",
    "                        ingredients_list.append(next_element.get_text())\n",
    "                    count += 1\n",
    "                    next_element = next_element.find_next()\n",
    "                \n",
    "                if ingredients_list:\n",
    "                    ingredient_sections.append(\" \".join(ingredients_list))\n",
    "    \n",
    "    # Compile the content to send to the LLM\n",
    "    if ingredient_sections:\n",
    "        content_for_llm = \"\\n\".join(ingredient_sections)\n",
    "    else:\n",
    "        # Fallback: use page title and first part of the content\n",
    "        title = soup.title.string if soup.title else \"Recipe\"\n",
    "        content_for_llm = f\"{title}\\n\\n{soup.get_text()[:4000]}\"\n",
    "    \n",
    "    # Create prompt for the LLM\n",
    "    recipe_context = f\" for {recipe_name}\" if recipe_name else \"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract only the ingredients list{recipe_context} from the following text.\n",
    "    Format each ingredient on a new line with quantities. Be precise and include only actual ingredients.\n",
    "    Do not include cooking instructions, equipment, or other non-ingredient information.\n",
    "    \n",
    "    Text:\n",
    "    {content_for_llm}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get ingredients from LLM\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error extracting ingredients: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_page(html_content, current_url):\n",
    "      \"\"\"Extract all links from a page\"\"\"\n",
    "      soup = BeautifulSoup(html_content, 'html.parser')\n",
    "      links = []\n",
    "      \n",
    "      for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute\n",
    "            absolute_url = urljoin(current_url, href)\n",
    "            # Filter out non-HTTP URLs (like javascript:, mailto:, etc)\n",
    "            if absolute_url.startswith(('http://', 'https://')):\n",
    "                  links.append(absolute_url)\n",
    "            \n",
    "      return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a901958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(website_url,recipe_query=None):\n",
    "        \"\"\"Crawl the website to find recipe pages\"\"\"\n",
    "        queue = [website_url]\n",
    "        pages_visited = 0\n",
    "        \n",
    "        print(f\"Starting crawl of {website_url}\")\n",
    "        max_pages = 20\n",
    "        recipe_urls = []\n",
    "        visited_urls = set()\n",
    "        while queue and pages_visited < max_pages and len(recipe_urls) < 10:\n",
    "            current_url = queue.pop(0)\n",
    "            \n",
    "            if current_url in visited_urls:\n",
    "                continue\n",
    "                \n",
    "            visited_urls.add(current_url)\n",
    "            pages_visited += 1\n",
    "            \n",
    "            print(f\"Visiting page {pages_visited}/{max_pages}: {current_url}\")\n",
    "            delay = 1\n",
    "            try:\n",
    "                # Add a delay to be respectful to the server\n",
    "                time.sleep(delay)\n",
    "                \n",
    "                # Fetch the page\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "                }\n",
    "                response = requests.get(current_url, headers=headers, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Get links from the page\n",
    "                links = get_links_from_page(response.text, current_url)\n",
    "                \n",
    "                # Check if current page is a recipe\n",
    "                if is_likely_recipe_url(current_url):\n",
    "                    # If we have a specific recipe query, check if it matches\n",
    "                    if recipe_query:\n",
    "                        # Use LLM to determine if this page matches the query\n",
    "                        if page_matches_recipe_query(response.text, recipe_query):\n",
    "                            recipe_urls.append(current_url)\n",
    "                            print(f\"Found matching recipe: {current_url}\")\n",
    "                    else:\n",
    "                        # Without a specific query, add all likely recipe pages\n",
    "                        recipe_urls.append(current_url)\n",
    "                        print(f\"Found recipe: {current_url}\")\n",
    "                \n",
    "                # Add new links to the queue\n",
    "                for link in links:\n",
    "                    if is_valid_url(link) and link not in queue:\n",
    "                        if is_likely_recipe_url(link):\n",
    "                            # Prioritize links that look like recipes\n",
    "                            queue.insert(0, link)\n",
    "                        else:\n",
    "                            queue.append(link)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {current_url}: {str(e)}\")\n",
    "                \n",
    "        print(f\"Crawl completed. Visited {len(visited_urls)} pages, found {len(recipe_urls)} recipes.\")\n",
    "        return recipe_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa65a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_extract_recipe(website_url, recipe_query=None, max_pages=15):\n",
    "    \"\"\"Find recipes on a website and extract ingredients\"\"\"\n",
    "    # Initialize crawler\n",
    "    # crawler = RecipeCrawler(website_url, max_pages=max_pages)\n",
    "    \n",
    "    # Crawl the website to find recipe pages\n",
    "    recipe_urls = crawl(website_url, recipe_query)\n",
    "    \n",
    "    if not recipe_urls:\n",
    "        return \"No recipe pages found on this website.\"\n",
    "    \n",
    "    # Extract ingredients from each recipe page\n",
    "    results = []\n",
    "    \n",
    "    for url in recipe_urls[:3]:  # Limit to first 3 recipes for testing\n",
    "        try:\n",
    "            print(f\"\\nExtracting ingredients from: {url}\")\n",
    "            # Fetch the page\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get page title for recipe name\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.title.string if soup.title else \"Unknown Recipe\"\n",
    "            \n",
    "            # Extract ingredients\n",
    "            ingredients = extract_ingredients_from_html(response.text, recipe_query)\n",
    "            \n",
    "            results.append({\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'ingredients': ingredients\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing recipe at {url}: {str(e)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9b5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_matches_recipe_query(self, page_content, recipe_query):\n",
    "        \"\"\"Use LLM to determine if a page matches the recipe query\"\"\"\n",
    "        # Extract page title and a snippet\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        title = soup.title.string if soup.title else \"No title\"\n",
    "        \n",
    "        # Get page text snippets\n",
    "        text_content = soup.get_text(separator=' ', strip=True)\n",
    "        snippet = text_content[:1000] + \"...\" if len(text_content) > 1000 else text_content\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        I'm looking for a recipe about \"{recipe_query}\".\n",
    "        \n",
    "        Page title: {title}\n",
    "        \n",
    "        Page content snippet: \n",
    "        {snippet}\n",
    "        \n",
    "        Based only on this information, answer with exactly \"YES\" if this page likely contains a recipe for {recipe_query}, \n",
    "        or exactly \"NO\" if it's unlikely or unrelated. Only respond with YES or NO.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            result = response.content.strip().upper()\n",
    "            return \"YES\" in result\n",
    "        except Exception as e:\n",
    "            print(f\"Error using LLM to check page: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dd39f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f1365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbddee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_url(self, url):\n",
    "      \"\"\"Check if URL belongs to the same domain and is not already visited\"\"\"\n",
    "      parsed = urlparse(url)\n",
    "      return parsed.netloc == self.domain and url not in self.visited_urls\n",
    "\n",
    "def is_likely_recipe_url(self, url):\n",
    "      \"\"\"Heuristic to determine if a URL likely points to a recipe\"\"\"\n",
    "      recipe_indicators = ['recipe', 'recipes', 'dish', 'meal', 'cook', \n",
    "                        'bake', 'food', 'ingredient', 'cuisine']\n",
    "      \n",
    "      url_lower = url.lower()\n",
    "      # Check if URL contains recipe indicators\n",
    "      return any(indicator in url_lower for indicator in recipe_indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ab1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_website = \"https://www.allrecipes.com/\"\n",
    "recipe_query = \"chocolate cake\"\n",
    "\n",
    "print(f\"Looking for {recipe_query} recipes on {test_website}\")\n",
    "results = find_and_extract_recipe(test_website, recipe_query, max_pages=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, recipe in enumerate(results):\n",
    "    print(f\"\\nRecipe {i+1}: {recipe['title']}\")\n",
    "    print(f\"URL: {recipe['url']}\")\n",
    "    print(\"Ingredients:\")\n",
    "    print(recipe['ingredients'])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
