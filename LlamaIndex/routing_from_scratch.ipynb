{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "choices = [\n",
    "    \"Useful for questions related to apples\",\n",
    "    \"Useful for questions related to oranges\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_choice_str(choices):\n",
    "    choices_str = \"\\n\\n\".join(\n",
    "        [f\"{idx+1}. {c}\" for idx, c in enumerate(choices)]\n",
    "    )\n",
    "    return choices_str\n",
    "\n",
    "\n",
    "choices_str = get_choice_str(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e004cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt0 = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n",
    "    \" {num_choices}), where each item in the list corresponds to a\"\n",
    "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n",
    "    \" only the choices above and not prior knowledge, return the top choices\"\n",
    "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n",
    "    \" most relevant to the question: '{query_str}'\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    azure_endpoint=\"https://oai-tss-sweden.openai.azure.com/\",\n",
    "    api_version=\"2024-02-01\",\n",
    "    api_key=\"f0c2aee69c914b35aa55732deb3532be\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9868dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_prompt(query_str):\n",
    "    fmt_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=2,\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    return fmt_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac32a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some choices are given below. It is provided in a numbered list (1 to 2), where each item in the list corresponds to a summary.\n",
      "---------------------\n",
      "1. Useful for questions related to apples\n",
      "\n",
      "2. Useful for questions related to oranges\n",
      "---------------------\n",
      "Using only the choices above and not prior knowledge, return the top choices (no more than 2, but only select what is needed) that are most relevant to the question: 'Can you tell me more about the amount of Vitamin C in apples'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Can you tell me more about the amount of Vitamin C in apples\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "print(fmt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc65494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vishwas\\AppData\\Local\\Temp\\ipykernel_19572\\3675367203.py:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(fmt_prompt)\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict(fmt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afb46e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Useful for questions related to apples'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7af9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "            azure_deployment=\"gpt-4o-mini\",\n",
    "            api_version=\"2024-02-01\",  \n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            azure_endpoint=\"https://oai-tss-sweden.openai.azure.com/\",\n",
    "            api_key=\"f0c2aee69c914b35aa55732deb3532be\",\n",
    "            streaming=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4223b4",
   "metadata": {},
   "source": [
    "Structured Response in Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ab1262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import fields\n",
    "from pydantic import BaseModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8480635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel):\n",
    "    choice: int\n",
    "    reason: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1db703ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"properties\": {\n",
      "    \"choice\": {\n",
      "      \"title\": \"Choice\",\n",
      "      \"type\": \"integer\"\n",
      "    },\n",
      "    \"reason\": {\n",
      "      \"title\": \"Reason\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"choice\",\n",
      "    \"reason\"\n",
      "  ],\n",
      "  \"title\": \"Answer\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(Answer.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3db81ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.types import BaseOutputParser\n",
    "FORMAT_STR = \"\"\"The output should be formatted as a JSON instance that conforms to \n",
    "the JSON schema below. \n",
    "\n",
    "Here is the output schema:\n",
    "{\n",
    "  \"type\": \"array\",\n",
    "  \"items\": {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "      \"choice\": {\n",
    "        \"type\": \"integer\"\n",
    "      },\n",
    "      \"reason\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    \"required\": [\n",
    "      \"choice\",\n",
    "      \"reason\"\n",
    "    ],\n",
    "    \"additionalProperties\": false\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee6c66",
   "metadata": {},
   "source": [
    "\n",
    "If we want to put FORMAT_STR as part of an f-string as part of a prompt template, then we'll need to escape the curly braces so that they don't get treated as template variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f0c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _escape_curly_braces(input_string: str) -> str:\n",
    "    # Replace '{' with '{{' and '}' with '}}' to escape curly braces\n",
    "    escaped_string = input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return escaped_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e7433",
   "metadata": {},
   "source": [
    "We now define a simple parsing function to extract out the JSON string from the LLM response (by searching for square brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1a11617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _marshal_output_to_json(output: str) -> str:\n",
    "    output = output.strip()\n",
    "    left = output.find(\"[\")\n",
    "    right = output.find(\"]\")\n",
    "    output = output[left : right + 1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fab8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class RouterOutputParser(BaseOutputParser):\n",
    "    def parse(self, output: str) -> List[Answer]:\n",
    "        \"\"\"Parse string.\"\"\"\n",
    "        json_output = _marshal_output_to_json(output)\n",
    "        json_dicts = json.loads(json_output)\n",
    "        answers = [Answer.from_dict(json_dict) for json_dict in json_dicts]\n",
    "        return answers\n",
    "\n",
    "    def format(self, prompt_template: str) -> str:\n",
    "        return prompt_template + \"\\n\\n\" + _escape_curly_braces(FORMAT_STR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df9b7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = RouterOutputParser()\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def route_query(\n",
    "    query_str: str, choices: List[str], output_parser: RouterOutputParser\n",
    "):\n",
    "    choices_str\n",
    "\n",
    "    fmt_base_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=len(choices),\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    fmt_json_prompt = output_parser.format(fmt_base_prompt)\n",
    "\n",
    "    raw_output = llm.complete(fmt_json_prompt)\n",
    "    parsed = output_parser.parse(str(raw_output))\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0db8d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"Represents a single choice with a reason.\"\n",
    "    choice: int\n",
    "    reason: str\n",
    "\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    \"\"\"Represents a list of answers.\"\"\"\n",
    "\n",
    "    answers: List[Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cdb40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Answer': {'description': 'Represents a single choice with a reason.',\n",
       "   'properties': {'choice': {'title': 'Choice', 'type': 'integer'},\n",
       "    'reason': {'title': 'Reason', 'type': 'string'}},\n",
       "   'required': ['choice', 'reason'],\n",
       "   'title': 'Answer',\n",
       "   'type': 'object'}},\n",
       " 'description': 'Represents a list of answers.',\n",
       " 'properties': {'answers': {'items': {'$ref': '#/$defs/Answer'},\n",
       "   'title': 'Answers',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answers'],\n",
       " 'title': 'Answers',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Answers.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee1eeb64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishwas\\miniconda3\\envs\\mcondaenv\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:42\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     41\u001b[39m     llm = OpenAI()\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[43mvalidate_openai_api_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishwas\\miniconda3\\envs\\mcondaenv\\Lib\\site-packages\\llama_index\\llms\\openai\\utils.py:713\u001b[39m, in \u001b[36mvalidate_openai_api_key\u001b[39m\u001b[34m(api_key)\u001b[39m\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
      "\u001b[31mValueError\u001b[39m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprogram\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIPydanticProgram\n\u001b[32m      2\u001b[39m router_prompt1 = router_prompt0.partial_format(\n\u001b[32m      3\u001b[39m     num_choices=\u001b[38;5;28mlen\u001b[39m(choices),\n\u001b[32m      4\u001b[39m     max_outputs=\u001b[38;5;28mlen\u001b[39m(choices),\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m program = \u001b[43mOpenAIPydanticProgram\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAnswers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouter_prompt1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m query_str = \u001b[33m\"\u001b[39m\u001b[33mWhat are the health benefits of eating orange peels?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m output = program(context_list=choices_str, query_str=query_str)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishwas\\miniconda3\\envs\\mcondaenv\\Lib\\site-packages\\llama_index\\program\\openai\\base.py:121\u001b[39m, in \u001b[36mOpenAIPydanticProgram.from_defaults\u001b[39m\u001b[34m(cls, output_cls, prompt_template_str, prompt, llm, verbose, allow_multiple, tool_choice, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_defaults\u001b[39m(\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m     **kwargs: Any,\n\u001b[32m    120\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mOpenAIPydanticProgram\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     llm = llm \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, OpenAI):\n\u001b[32m    124\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    125\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOpenAIPydanticProgram only supports OpenAI LLMs. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(llm)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    126\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishwas\\miniconda3\\envs\\mcondaenv\\Lib\\site-packages\\llama_index\\core\\settings.py:36\u001b[39m, in \u001b[36m_Settings.llm\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[38;5;28mself\u001b[39m._llm = \u001b[43mresolve_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdefault\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mself\u001b[39m._llm.callback_manager = \u001b[38;5;28mself\u001b[39m._callback_manager\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Vishwas\\miniconda3\\envs\\mcondaenv\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:49\u001b[39m, in \u001b[36mresolve_llm\u001b[39m\u001b[34m(llm, callback_manager)\u001b[39m\n\u001b[32m     44\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     45\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`llama-index-llms-openai` package not found, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     47\u001b[39m         )\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     50\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCould not load OpenAI model. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m******\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         )\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     60\u001b[39m     splits = llm.split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
     ]
    }
   ],
   "source": [
    "from llama_index.program.openai import OpenAIPydanticProgram\n",
    "router_prompt1 = router_prompt0.partial_format(\n",
    "    num_choices=len(choices),\n",
    "    max_outputs=len(choices),\n",
    ")\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Answers,\n",
    "    prompt=router_prompt1,\n",
    "    verbose=True,\n",
    ")\n",
    "query_str = \"What are the health benefits of eating orange peels?\"\n",
    "output = program(context_list=choices_str, query_str=query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92cf7f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['num_choices', 'context_list', 'max_outputs', 'query_str'] kwargs={'num_choices': 2, 'max_outputs': 2} output_parser=None template_var_mappings=None function_mappings=None template=\"Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\\n\"\n"
     ]
    }
   ],
   "source": [
    "print(router_prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94734a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
