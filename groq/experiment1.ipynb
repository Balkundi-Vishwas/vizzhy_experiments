{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500934be",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc4c3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have become a crucial component in the field of Natural Language Processing (NLP). The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable applications to respond quickly to user input, providing a seamless and interactive experience. This is particularly important for applications such as virtual assistants, chatbots, and language translation software.\n",
      "2. **Real-time Processing**: Fast language models can process and analyze large amounts of text data in real-time, allowing for applications such as sentiment analysis, named entity recognition, and machine translation to be performed efficiently.\n",
      "3. **Increased Efficiency**: Fast language models can significantly reduce the computational resources required for NLP tasks, making them more energy-efficient and cost-effective. This is particularly important for large-scale applications and data centers.\n",
      "4. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by providing faster and more accurate NLP capabilities, enabling them to respond quickly to changing market conditions and customer needs.\n",
      "5. **Enhanced Automation**: Fast language models can automate many NLP tasks, such as data preprocessing, feature extraction, and model training, freeing up human resources for more strategic and creative tasks.\n",
      "6. **Better Support for Edge Devices**: Fast language models can be deployed on edge devices, such as smartphones, smart home devices, and IoT devices, allowing for NLP capabilities to be performed locally, reducing latency and improving overall performance.\n",
      "7. **Improved Model Training**: Fast language models can be used to train larger and more complex models, enabling researchers to explore new architectures and techniques that can lead to breakthroughs in NLP research.\n",
      "8. **Enhanced Multilingual Support**: Fast language models can be used to develop multilingual models that can support multiple languages, enabling organizations to expand their reach and cater to diverse customer bases.\n",
      "\n",
      "Some of the key applications of fast language models include:\n",
      "\n",
      "1. **Virtual assistants**: Fast language models are used in virtual assistants such as Siri, Alexa, and Google Assistant to provide quick and accurate responses to user queries.\n",
      "2. **Language translation**: Fast language models are used in language translation software to provide real-time translations and improve the overall translation quality.\n",
      "3. **Sentiment analysis**: Fast language models are used in sentiment analysis applications to analyze large amounts of text data and provide insights into customer opinions and preferences.\n",
      "4. **Text summarization**: Fast language models are used in text summarization applications to summarize large documents and provide concise summaries of key points.\n",
      "5. **Chatbots**: Fast language models are used in chatbots to provide quick and accurate responses to user queries and improve the overall user experience.\n",
      "\n",
      "Overall, fast language models have the potential to revolutionize the field of NLP and enable a wide range of applications that can transform the way we interact with language and machines.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e29cf",
   "metadata": {},
   "source": [
    "for asynchronous processing\n",
    "1. prepare jsonl file which has\n",
    "      custom_id: Your unique identifier for tracking the batch request\n",
    "      method: The HTTP method (currently POST only)\n",
    "      url: The API endpoint to call (/v1/chat/completions or /v1/audio/transcriptions)\n",
    "      body: The parameters of your request matching our synchronous API format. See our API Reference\n",
    "\n",
    "2. upload the batch file to groq\n",
    "3. create the batch job\n",
    "4. check the batch status\n",
    "5. retrive batch results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e657c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests # pip install requests first!\n",
    "\n",
    "def upload_file_to_groq(api_key, file_path):\n",
    "    url = \"https://api.groq.com/openai/v1/files\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    # Prepare the file and form data\n",
    "    files = {\n",
    "        \"file\": (\"batch_file.jsonl\", open(file_path, \"rb\"))\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"purpose\": \"batch\"\n",
    "    }\n",
    "    \n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Usage example\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "file_path = \"batch_file.jsonl\"  # Path to your JSONL file\n",
    "\n",
    "try:\n",
    "    result = upload_file_to_groq(api_key, file_path)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests # pip install requests first! \n",
    "\n",
    "def create_batch(api_key, input_file_id):\n",
    "    url = \"https://api.groq.com/openai/v1/batches\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": \"/v1/chat/completions\",\n",
    "        \"completion_window\": \"24h\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# Usage example\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "file_id = \"file_01jh6x76wtemjr74t1fh0faj5t\" # replace with your `id` from file upload API response object\n",
    "\n",
    "try:\n",
    "    result = create_batch(api_key, file_id)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests # pip install requests first! \n",
    "\n",
    "def create_batch(api_key, input_file_id):\n",
    "    url = \"https://api.groq.com/openai/v1/batches\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"input_file_id\": input_file_id,\n",
    "        \"endpoint\": \"/v1/chat/completions\",\n",
    "        \"completion_window\": \"24h\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "# Usage example\n",
    "api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "file_id = \"file_01jh6x76wtemjr74t1fh0faj5t\" # replace with your `id` from file upload API response object\n",
    "\n",
    "try:\n",
    "    result = create_batch(api_key, file_id)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
