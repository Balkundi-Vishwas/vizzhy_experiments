{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0554566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a251c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI( api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "198b7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results=2,api_key=os.environ.get(\"TAVILY_API_KEY\"),include_domains = ['www.healthyschoolrecipes.com'])\n",
    "search_results = search.invoke(\"what are the ingredients recipe for Stromboli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e07a5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" Do NOT answer based on your own knowledge or assumptions.\"\n",
    "        \" ONLY respond if the tool provides a valid and relevant result.\"\n",
    "        \" If the tool fails or returns nothing useful, say you cannot find the answer.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a981c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_system_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m search = TavilySearchResults(max_results=\u001b[32m2\u001b[39m,include_domains = [\u001b[33m'\u001b[39m\u001b[33mwww.indianhealthyrecipes.com\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mwww.allrecipes.com\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m tools = [search]\n\u001b[32m      8\u001b[39m agent_executor = create_react_agent(model, tools,\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m                                     prompt=\u001b[43mmake_system_prompt\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'make_system_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o\", model_provider=\"openai\",api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "search = TavilySearchResults(max_results=2,include_domains = ['www.healthyschoolrecipes.com'])\n",
    "tools = [search]\n",
    "\n",
    "agent_executor = create_react_agent(model, tools,\n",
    "                                    prompt=make_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b7ae256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot find the detailed ingredients for an idly recipe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1. Setup Tavily tool\n",
    "search = TavilySearchResults(\n",
    "    max_results=2,\n",
    "    include_domains=[\"www.healthyschoolrecipes.com\"]  # Optional\n",
    ")\n",
    "\n",
    "# 2. Define system prompt\n",
    "def make_system_prompt():\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" Do NOT answer based on your own knowledge or assumptions.\"\n",
    "        \" ONLY respond if the tool provides a valid and relevant result.\"\n",
    "        \" If the tool fails or returns nothing useful, say you cannot find the answer.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "         )\n",
    "\n",
    "# 3. Setup LLM and agent\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model = init_chat_model(\"gpt-4o\", model_provider=\"openai\",api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# Create agent with tool\n",
    "agent_executor = create_react_agent(\n",
    "    model,\n",
    "    tools=[search],\n",
    "    prompt=make_system_prompt()\n",
    ")\n",
    "\n",
    "# 4. Run query\n",
    "query = \"Fetch all the detailed ingredients for any idly recipe?\"\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=query)]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde1e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    "    )\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to yes or a no.\",\n",
    "    )\n",
    "\n",
    "def toolagent(state: MessagesState):\n",
    "\n",
    "    query = state[\"messages\"][0].content\n",
    "    response = agent_executor.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "\n",
    "    context = response[\"messages\"][-1].content\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    # structured_llm_grader = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "    # system = \"\"\"You are a grader assessing whether the answer addresses / resolves a question \\n \n",
    "    #     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "    # answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    #     [\n",
    "    #         (\"system\", system),\n",
    "    #         (\"human\", \"User question: \\n\\n {question} \\n\\n LLM answer: {answer}\"),\n",
    "    #     ]\n",
    "    # )\n",
    "\n",
    "    # answer_grader = answer_prompt | structured_llm_grader\n",
    "    # answer_grader.invoke({\"question\": query, \"answer\": context })\n",
    "    prompt = GRADE_PROMPT.format(question=query, context=context)\n",
    "    response = (\n",
    "        llm\n",
    "        # highlight-next-line\n",
    "        .with_structured_output(RouteQuery).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return {\"question\": query, \"generation\": context}\n",
    "    else:\n",
    "        return {\"question\": query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1ac25",
   "metadata": {},
   "source": [
    "change of plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bbc6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1. Setup Tavily tool\n",
    "\n",
    "def toolagent(state):\n",
    "    search = TavilySearchResults(\n",
    "        max_results=2,\n",
    "        include_domains=['www.indianhealthyrecipes.com','www.allrecipes.com']\n",
    "    )\n",
    "\n",
    "    # 2. Define system prompt\n",
    "    def make_system_prompt():\n",
    "        return (\n",
    "            \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "            \" Use the provided tools to progress towards answering the question.\"\n",
    "            \" Do NOT answer based on your own knowledge or assumptions.\"\n",
    "            \" ONLY respond if the tool provides a valid and relevant result.\"\n",
    "            \" If the tool fails or returns nothing useful, say you cannot find the answer.\"\n",
    "            \" Also site the source where the recipe had been taken from.\"\n",
    "            )\n",
    "\n",
    "    # 3. Setup LLM and agent\n",
    "    # llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    model = init_chat_model(\"gpt-4o\", model_provider=\"openai\",api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    # Create agent with tool\n",
    "    agent_executor = create_react_agent(\n",
    "        model,\n",
    "        tools=[search],\n",
    "        prompt=make_system_prompt()\n",
    "    )\n",
    "\n",
    "    # 4. Run query\n",
    "    query = state['question']\n",
    "    response = agent_executor.invoke(\n",
    "        {\"messages\": [HumanMessage(content=query)]}\n",
    "    )\n",
    "\n",
    "    # print(response[\"messages\"][-1].content)\n",
    "    return {\"generation\":response[\"messages\"][-1].content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042bd526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_agent(state):\n",
    "    query= state['question']\n",
    "    client = OpenAI( api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    completion = client.chat.completions.create(\n",
    "    # model=\"gpt-4o\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # model=\"gpt-4.1\",\n",
    "    # model=\"gpt-4.1-mini\",\n",
    "    # model=\"gpt-4.1-nano\",\n",
    "    # model=\"o3-mini\",\n",
    "    messages=[\n",
    "            {\"role\": \"system\",\n",
    "            \"content\":\"You are a cooking agent, Help me generate the ingredients for the user provided recipe\"           \n",
    "            },\n",
    "            {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": f\"\"\"{query}\"\"\"\n",
    "                  \n",
    "            },\n",
    "      ],\n",
    "      )\n",
    "    return {\n",
    "        \"generation\": completion.choices[0].message.content\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04261014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "GRADE_PROMPT = (\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n",
    "    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n",
    "    \"Here is the user question: {question} \\n\"\n",
    "    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n",
    "    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    "    )\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to yes or a no.\",\n",
    "    )\n",
    "\n",
    "def answersatisfied(state):\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    query = state['question']\n",
    "    generation = state['generation']\n",
    "    prompt = GRADE_PROMPT.format(question=query, context=generation)\n",
    "    response = (\n",
    "        llm\n",
    "        # highlight-next-line\n",
    "        .with_structured_output(RouteQuery).invoke(\n",
    "            [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "    )\n",
    "    score = response.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        return \"not useful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c0ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(toolagent)\n",
    "workflow.add_node(regular_agent)\n",
    "\n",
    "workflow.add_edge(START, \"toolagent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"toolagent\",\n",
    "    answersatisfied,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\":\"regular_agent\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"regular_agent\",END)\n",
    "app = workflow.compile()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 'toolagent':\n",
      "\n",
      "---\n",
      "\n",
      "Here are the detailed ingredients for two different Baked Shrimp Scampi recipes:\n",
      "\n",
      "1. **[Shrimp Scampi Bake Recipe from Allrecipes](https://www.allrecipes.com/recipe/25874/shrimp-scampi-bake/)**\n",
      "\n",
      "   Ingredients:\n",
      "   - 2 pounds medium raw shrimp, shelled, deveined, with tails attached\n",
      "   - 1 cup butter\n",
      "   - 2 tablespoons Dijon mustard\n",
      "   - 1 tablespoon fresh lemon juice\n",
      "   - 1 tablespoon chopped garlic\n",
      "   - 1 tablespoon chopped fresh parsley\n",
      "\n",
      "2. **[Baked Shrimp Scampi Recipe from Allrecipes](https://www.allrecipes.com/recipe/229959/baked-shrimp-scampi/)**\n",
      "\n",
      "   Ingredients:\n",
      "   - 1 pound large shrimp, peeled and deveined\n",
      "   - 1 cup unsalted butter\n",
      "   - ¼ cup white wine\n",
      "   - 2 tablespoons lemon juice\n",
      "   - 2 tablespoons dried parsley\n",
      "   - 1 teaspoon cayenne pepper\n",
      "   - 2 tablespoons minced garlic\n",
      "   - ½ cup Italian-seasoned bread crumbs\n",
      "\n",
      "You may choose either recipe based on your preference for ingredients.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Fetch all the detailed ingredients for Baked Shrimp Scampi?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "print(value[\"generation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cef3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
