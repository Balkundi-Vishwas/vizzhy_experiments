{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a251c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI( api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198b7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results=2,api_key=os.environ.get(\"TAVILY_API_KEY\"),include_domains = ['www.healthyschoolrecipes.com'])\n",
    "search_results = search.invoke(\"what are the ingredients recipe for Stromboli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e07a5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_system_prompt() -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" Do NOT answer based on your own knowledge or assumptions.\"\n",
    "        \" ONLY respond if the tool provides a valid and relevant result.\"\n",
    "        \" If the tool fails or returns nothing useful, say you cannot find the answer.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145a981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o\", model_provider=\"openai\",api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "search = TavilySearchResults(max_results=2,include_domains = ['www.healthyschoolrecipes.com'])\n",
    "tools = [search]\n",
    "\n",
    "agent_executor = create_react_agent(model, tools,\n",
    "                                    prompt=make_system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6590e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b7ae256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I couldn't find any specific information about the detailed ingredients for an idly recipe.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 1. Setup Tavily tool\n",
    "search = TavilySearchResults(\n",
    "    max_results=2,\n",
    "    include_domains=[\"www.healthyschoolrecipes.com\"]  # Optional\n",
    ")\n",
    "\n",
    "# 2. Define system prompt\n",
    "def make_system_prompt():\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" Do NOT answer based on your own knowledge or assumptions.\"\n",
    "        \" ONLY respond if the tool provides a valid and relevant result.\"\n",
    "        \" If the tool fails or returns nothing useful, say you cannot find the answer.\"\n",
    "        \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "        \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "         )\n",
    "\n",
    "# 3. Setup LLM and agent\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "model = init_chat_model(\"gpt-4o\", model_provider=\"openai\",api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "# Create agent with tool\n",
    "agent_executor = create_react_agent(\n",
    "    model,\n",
    "    tools=[search],\n",
    "    prompt=make_system_prompt()\n",
    ")\n",
    "\n",
    "# 4. Run query\n",
    "query = \"Fetch all the detailed ingredients for any idly recipe?\"\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=query)]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dde1e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(binary_score='no')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to yes or a no.\",\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(RouteQuery)\n",
    "system = \"\"\"You are a grader assessing whether the answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": query, \"generation\": response[\"messages\"][-1].content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "\n",
    "workflow = StateGraph(GraphState)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcondaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
